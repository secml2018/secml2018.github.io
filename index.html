<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="NeurIPS 2018 Workshop on Security in Machine Learning">

  <title>NeurIPS 2018 Workshop on Security in Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5">NeurIPS 2018 Workshop on Security in Machine Learning</h1>
  <p class="mb-0"><b>Date:</b> December 7, 2018 (Friday)</p>
  <p class="mb-0"><b>Location:</b> Montreal, Canada (co-located with <a href="https://nips.cc/" target="_blank">NeurIPS 2018</a>)</p>
  <p class="mb-0"><b>Contact:</b> secml2018-org@googlegroups.com (this will email all organizers)</p>
  <p><b>Room:</b> 513DEF</p>
  <p>
    <i>Abstract</i>—There is growing recognition that ML exposes new vulnerabilities in software systems. Some of the threat vectors explored so far include training data poisoning, adversarial examples or model extraction. Yet, the technical community's understanding of the nature and extent of the resulting vulnerabilities remains limited. This is due in part to (1) the large attack surface exposed by ML algorithms because they were designed for deployment in benign environments---as exemplified by the IID assumption for training and test data, (2) the limited availability of theoretical tools to analyze generalization, (3) the lack of reliable confidence estimates. In addition, the majority of work so far has focused on a small set of application domains and threat models.
  </p>
  <p>
    This workshop will bring together experts from the computer security and machine learning communities in an attempt to highlight recent work that contribute to address these challenges. Our agenda will complement contributed papers with invited speakers. The latter will emphasize connections between ML security and other research areas such as accountability or formal verification, as well as stress social aspects of ML misuses. We hope this will help identify fundamental directions for future cross-community collaborations, thus charting a path towards secure and trustworthy ML.
  </p>
  <h2>Sponsor</h2>
  <p>Thank you to the Open Philanthropy Project for sponsoring this event. Their grant will fund a best paper award as well as support for travel.</p>

  
  <h2>Schedule</h2>
  <p>The following is a tentative schedule and is subject to change prior to the workshop.</p>

  <table class="table table-sm">
    <tbody>
    <tr>
      <th scope="row">8:45am</th>
      <td>Opening Remarks</td>
      <td>TBD</td>
    </tr>

    <tr><th scope="row" colspan="3">Session 1: Provable methods for secure ML</th></tr>
    <tr>
      <th scope="row">9:00am</th>
      <td>Contributed Talk #1: Sever: A Robust Meta-Algorithm for Stochastic Optimization</td>
      <td>Jerry Li</td>
    </tr>
    <tr>
      <th scope="row">9:15am</th>
      <td>Invited Talk #1: Semidefinite relaxations for certifying robustness to adversarial examples</td>
      <td><a href="https://stanford.edu/~aditir/" target="_blank">Aditi Raghunathan</a></td>
    </tr>
    <tr>
      <th scope="row">9:45am</th>
      <td>Contributed Talk #2: On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models</td>
      <td>TBD</td>
    </tr>
    <tr><th scope="row" colspan="3">Poster session 1</th></tr>
    <tr>
      <th scope="row">10:00am</th>
      <td>Poster Session followed by coffee break</td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 2: ML security and society</th></tr>
    <tr>
      <th scope="row">11:00am</th>
      <td>Keynote: A Sociotechnical Approach to Security in Machine Learning</td>
      <td><a href="http://www.danah.org/" target="_blank">danah boyd</a></td>
    </tr>
    <tr>
      <th scope="row">11:45am</th>
      <td>Contributed Talk #3: Law and Adversarial Machine Learning</td>
      <td>TBD</td>
    </tr>

    <tr><th scope="row" colspan="3">Lunch break</th></tr>

    <tr><th scope="row" colspan="3">Session 3: Attacks and interpretability</th></tr>
    <tr>
      <th scope="row">1:30pm</th>
      <td>Invited Talk #2: Interpretability for when NOT to use machine learning</td>
      <td><a href="https://beenkim.github.io/" target="_blank">Been Kim</a></td>
    </tr>
    <tr>
      <th scope="row">2:00pm</th>
      <td>Contributed Talk #5: Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures</td>
      <td>Csaba Szepesvari</td>
    </tr>
    <tr>
      <th scope="row">2:15pm</th>
      <td>Invited Talk #3: Semantic Adversarial Examples</td>
      <td><a href="http://pages.cs.wisc.edu/~jha/" target="_blank">Somesh Jha</a></td>
    </tr>
    <tr><th scope="row" colspan="3">Poster session 2</th></tr>
    <tr>
      <th scope="row">2:45pm</th>
      <td>Poster Session followed by coffee break</td>
      <td></td>
    </tr>
    <tr><th scope="row" colspan="3">Session 4: ML security from a formal verification perspective</th></tr>
    <tr>
      <th scope="row">4:15pm</th>
      <td>Invited Talk #4: Safety verification for neural networks with provable guarantees</td>
      <td><a href="http://www.cs.ox.ac.uk/marta.kwiatkowska/" target="_blank">Marta Kwiatkowska</a></td>
    </tr>
    <tr>
      <th scope="row">4:45pm</th>
      <td>Contributed Talk #4: Model Poisoning Attacks in Federated Learning</td>
      <td>TBD</td>
    </tr>
    </tbody>
  </table>
	
	<h2>Accepted papers</h2>
<h4>Research track:</h4>
<ul>
<li><strong>
Sever: A Robust Meta-Algorithm for Stochastic Optimization <span class="badge badge-success">morning</span>
</strong> Ilias Diakonikolas (USC); Gautam Kamath (MIT); Daniel M Kane (UCSD); Jerry Li (MIT); Jacob Steinhardt (Stanford); Alistair Stewart (University of Southern California) [<a href="https://arxiv.org/abs/1803.02815">paper</a>]</li>
<li><strong>
Evading classifiers in discrete domains with provable optimality guarantees <span class="badge badge-info">afternoon</span>
</strong>Bogdan Kulynych (EPFL); Jamie Hayes (University College London); Nikita Samarin (UC Berkeley); Carmela Troncoso (EPFL) [<a href="https://arxiv.org/abs/1810.10939">paper</a>]</li>
<li><strong>
Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples <span class="badge badge-info">afternoon</span>
</strong>Felix Kreuk, Assi Barak, Shir Aviv, Moran Baruch, Benny Pinkas, Joseph Keshet (Bar-Ilan University) [<a href="https://secml2018.github.io/malware.pdf">paper</a>].</li>
<li><strong>
A Surprising Density of Illusionable Natural Speech <span class="badge badge-info">afternoon</span>
</strong>Melody Y Guan (Stanford University); Gregory Valiant (Stanford University) [<a href="https://secml2018.github.io/A_Surprising_Density_of_Illusionable_Natural_Speech_SecML-2.pdf">paper</a>]</li>
<li><strong>
On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models <span class="badge badge-success">morning</span>
</strong>Sven Gowal (DeepMind); Krishnamurthy Dvijotham (DeepMind); Robert Stanforth (Deepmind); Rudy R Bunel (University of Oxford); Chongli Qin (DeepMind); Jonathan Uesato (DeepMind); Relja Arandjelovic (DeepMind); Timothy Arthur Mann (DeepMind); Pushmeet Kohli (DeepMind) [<a href="https://arxiv.org/abs/1810.12715">paper</a>]</li>
<li><strong>
Evaluating and Understanding the Robustness of Adversarial Logit Pairing <span class="badge badge-info">afternoon</span>
</strong>Logan Engstrom (MIT); Andrew Ilyas (MIT); Anish Athalye (Massachusetts Institute of Technology) [<a href="https://arxiv.org/abs/1807.10272">paper</a>]</li>
<li><strong>
Law and Adversarial Machine Learning <span class="badge badge-info">afternoon</span>
</strong>Ram Shankar Siva Kumar (Microsoft Azure Security); David R. O'Brien (Berkman Klein Center for Internet and Society); Kendra Albert (Harvard Law School); Salome Viljoen (Berkman Klein Center) [<a href="https://arxiv.org/abs/1810.10731">paper</a>]</li>
<li><strong>
How the Softmax Output is Misleading for Evaluating the Strength of Adversarial Examples <span class="badge badge-info">afternoon</span>
</strong>Utku Ozbulak (Ghent University); Wesley De Neve (Ghent University); Arnout Van Messem (Ghent University) [<a href="https://arxiv.org/abs/1811.08577">paper</a>]</li>
<li><strong>
Unknown Family Detection Based on Family-Invariant Representation <span class="badge badge-success">morning</span>
</strong>Toshiki Shibahara (NTT Secure Platform Laboratories); Daiki Chiba (NTT Secure Platform Laboratories); Mitsuaki Akiyama (NTT Secure Platform Laboratories); Kunio Hato (NTT Secure Platform Laboratories); Daniel Dalek (NTT Security); Masayuki Murata (Osaka University, Japan) [<a href="https://secml2018.github.io/shibahara_unknown_family_detection.pdf">paper</a>]</li>
<li><strong>
Decoupling Direction and Norm for Efficient Gradient-based L2 Adversarial Attacks <span class="badge badge-info">afternoon</span>
</strong>Jérôme Rony (ÉTS Montréal); Luiz Gustavo (Canada); Robert Sabourin (Canada); Eric Granger (École de technologie supérieure, Université du Québec)[<a href="https://secml2018.github.io/ddn_workshop_version.pdf">paper</a>]</li>
<li><strong>
Towards the first adversarially robust neural network model on MNIST <span class="badge badge-success">morning</span>
</strong>Lukas Schott (University of Tuebingen); Jonas Rauber (University of Tübingen); Matthias Bethge (University of Tübingen); Wieland Brendel (University of Tuebingen) [<a href="https://arxiv.org/abs/1805.09190">paper</a>]</li>
<li><strong>
Verification of deep probabilistic models <span class="badge badge-success">morning</span>
</strong>Krishnamurthy Dvijotham (DeepMind); Marta Garnelo (DeepMind); Alhussein Fawzi (Google Deepmind); Pushmeet Kohli (DeepMind)</li>
<li><strong>
A Statistical Approach to Assessing Neural Network Robustness <span class="badge badge-success">morning</span>
</strong>Stefan Webb; Tom Rainforth; Yee Whye Teh; M. Pawan Kumar (University of Oxford) [<a href="https://arxiv.org/abs/1811.07209">paper</a>]</li>
<li><strong>
Logit Pairing Methods Can Fool Gradient-Based Attacks <span class="badge badge-info">afternoon</span>
</strong>Marius Mosbach (Saarland University); Maksym Andriushchenko (Saarland University); Thomas Trost (Saarland University); Matthias Hein (University of Tuebingen); Dietrich Klakow (Saarland University) [<a href="https://arxiv.org/abs/1810.12042">paper</a>]</li>
<li><strong>
Adversarial Reprogramming of Neural Networks <span class="badge badge-info">afternoon</span>
</strong>Gamaleldin F Elsayed (Google Brain); Ian Goodfellow (Google Brain); Jascha Sohl-Dickstein (Google Brain) [<a href="https://arxiv.org/abs/1806.11146">paper</a>]</li>
<li><strong>
EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models <span class="badge badge-info">afternoon</span>
</strong>Hyrum Anderson (Endgame, Inc); Phil Roth (Endgame, Inc.) [<a href="https://arxiv.org/abs/1804.04637">paper</a>]</li>
<li><strong>
Adversarial Examples from Computational Constraints <span class="badge badge-success">morning</span>
</strong>Sebastien Bubeck (Microsoft Research); Yin Tat Lee (UW); Eric Price (University of Texas at Austin); Ilya Razenshteyn (Microsoft Research) [<a href="https://arxiv.org/abs/1805.10204">paper</a>]</li>
<li><strong>
Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures <span class="badge badge-info">afternoon</span>
</strong>Jonathan Uesato (DeepMind); Ananya Kumar (DeepMind); Csaba Szepesvari (DeepMind/University of Alberta); Pushmeet Kohli (DeepMind)[<a href="https://secml2018.github.io/adversarial_rl_workshop_final.pdf">paper</a>]</li>
<li><strong>
The Curse of Concentration in Robust Learning <span class="badge badge-success">morning</span>
</strong>Saeed Mahloujifar (University of Virginia); Dimitrios I Diochnos (University of Virginia); Mohammad Mahmoody (University of Virginia) [<a href="https://arxiv.org/abs/1809.03063">paper</a>]</li>
<li><strong>
DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules <span class="badge badge-success">morning</span>
</strong>Nicholas Frosst (Google); Sara Sabour (Google); Geoffrey Hinton (Google) [<a href="https://arxiv.org/abs/1811.06969">paper</a>]</li>
<li><strong>
Model Poisoning Attacks in Federated Learning <span class="badge badge-info">afternoon</span>
</strong>Arjun Nitin Bhagoji (Princeton University); Supriyo Chakraborty (IBM Research); Prateek Mittal (Princeton University); Seraphin Calo (IBM Research) [<a href=" http://arxiv.org/abs/1811.12470">paper</a>]</li>
<li><strong>
Targeted Adversarial Examples for Black Box Audio Systems <span class="badge badge-info">afternoon</span>
</strong>Rohan Taori (University of California, Berkeley); Amog Kamsetty (UC Berkeley); Brenton Chu (UC Berkeley); Nikita Vemuri (UC Berkeley) [<a href="https://arxiv.org/abs/1805.07820">paper</a>]</li>
<li><strong>
On the Sensitivity of Adversarial Robustness to Input Data Distributions <span class="badge badge-success">morning</span>
</strong>Gavin Weiguang Ding Yik Chau Lui Xiaomeng Jin Luyu Wang Ruitong Huang (Borealis AI)</li>
<li><strong>
PassGAN: A Deep Learning Approach for Password Guessing <span class="badge badge-info">afternoon</span>
</strong>Briland Hitaj (Stevens Institute of Technology); Paolo Gasti (NYIT); Giuseppe Ateniese (Stevens Institute of Technology); Fernando Perez-Cruz (ETH Zurich) [<a href="https://github.com/secml2018/secml2018.github.io/raw/master/PASSGAN_SECML2018.pdf">paper</a>]</li>
<li><strong>
Attend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models <span class="badge badge-info">afternoon</span>
</strong>Vasu Sharma (Carnegie Mellon University); Ankita Kalra (Carnegie Mellon University); Vaibhav Vaibhav (Carnegie Mellon University); Simral Chaudhary (Carnegie Mellon University); Labhesh Patel (Jumio Inc.); Louis-Philippe Morency (Carnegie Mellon University) [<a href="https://secml2018.github.io/attend.pdf">paper</a>]</li>
<li><strong>
Adversarial Examples as an Input-Fault Tolerance Problem <span class="badge badge-info">afternoon</span>
</strong>Angus Galloway (University of Guelph); Anna Golubeva (University of Waterloo); Graham Taylor (University of Guelph) [<a href="https://secml2018.github.io/advex_input_fault_tolerance_nips18_secml.pdf">paper</a>]</li>
<li><strong>
Towards Hiding Adversarial Examples from Network Interpretation <span class="badge badge-info">afternoon</span>
</strong>Akshayvarun Subramanya (UMBC); Vipin Pillai (UMBC); Hamed Pirsiavash (UMBC) [<a href="https://secml2018.github.io/towards_hiding.pdf">paper</a>]</li>
</ul>

<h4>Encore track:</h4>
<ul>
<li><strong>
PAC-learning in the presence of evasion adversaries <span class="badge badge-success">morning</span>
</strong>Daniel Cullina (Princeton University); Arjun Nitin Bhagoji (Princeton University); Prateek Mittal (Princeton University) [<a href="https://arxiv.org/abs/1806.01471">paper</a>]</li>
<li><strong>
Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring <span class="badge badge-success">morning</span>
</strong>Yossi Adi (Bar-Ilan University); Carsten Baum (Bar-Ilan University); Moustapha Cisse (Facebook AI Research); Benny Pinkas (Bar-Ilan University); Joseph Keshet (Dept. of Computer Science, Bar-Ilan University) [<a href="https://arxiv.org/abs/1802.04633">paper</a>]</li>
<li><strong>
Generating Natural Language Adversarial Examples <span class="badge badge-info">afternoon</span>
</strong>Yash Sharma (Cooper Union); Moustafa Alzantot (UCLA); Ahmed Elgohary (University of Maryland); Bo-Jhang Ho (UCLA); Mani Srivastava (UC Los Angeles); Kai-Wei Chang (UCLA) [<a href="https://arxiv.org/abs/1804.07998">paper</a>]</li>
</ul>


  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <div class="col-lg-1"></div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="nicolas.jpg" height="100px">
      <p>Nicolas Papernot<br /> (Chair)</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="florian.jpg" height="100px">
      <p>Florian Tramer<br /> (Co-chair)</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="kamalika.jpg" height="100px">
      <p>Kamalika Chaudhuri</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="mfredrik.jpg" height="100px">
      <p>Matt Fredrikson</p>
    </div>
    <div class="col-lg-2">
      <img class="rounded-circle" src="photo.png" height="100px">
      <p>Jacob Steinhardt</p>
    </div>
    <div class="col-lg-1"></div>
  </div>
	
	

<h2>Program Committee</h2>
<ul>
	<li>Aditi Raghunathan (Stanford University)</li>
	<li>Alexey Kurakin (Google Brain)</li>
	<li>Ananth Raghunathan (Google Brain)</li>
	<li>Anish Athalye (Massachusetts Institute of Technology)</li>
	<li>Arunesh Sinha (University of Michigan)</li>
	<li>Battista Biggio (University of Cagliari)</li>
	<li>Berkay Celik (Pennsylvania State University)</li>
	<li>Catherine Olsson (Google Brain)</li>
	<li>David Evans (University of Virginia)</li>
	<li>Dimitris Tsipras (Massachusetts Institute of Technology)</li>
	<li>Earlence Fernandes (University of Washington)</li>
	<li>Eric Wong (Carnegie Mellon University)</li>
	<li>Fartash Faghri (University of Toronto)</li>
	<li>Florian Tramer (Stanford University)</li>
	<li>Hadi Abdullah (University of Florida)</li>
	<li>Jamie Hayes (Unversity College London)</li>
	<li>Jonathan Uesato (DeepMind)</li>
	<li>Kassem Fawaz (University of Wisconsin-Madison)</li>
	<li>Kathrin Grosse (CISPA)</li>
	<li>Krishna Gummadi (MPI-SWS)</li>
	<li>Krishnamurthy Dvijotham (Deepmind)</li>
	<li>Matthew Wicker (University of Georgia)</li>
	<li>Nicholas Carlini (Google Brain)</li>
	<li>Nicolas Papernot (Google Brain)</li>
	<li>Octavian Suciu (University of Maryland)</li>
	<li>Pin-Yu Chen (IBM)</li>
	<li>Rudy Bunel (University of Oxford)</li>
	<li>Shreya Shankar (Stanford University)</li>
	<li>Suman Jana (Columbia University)</li>
	<li>Varun Chandrasekaran (University of Wisconsin-Madison)</li>
	<li>Xiaowei Huang (Liverpool University)</li>
	<li>Yanjun Qi (University of Virginia)</li>
	<li>Yigitcan Kaya (University of Maryland)</li>
	<li>Yizheng Chen (Georgia Tech)</li>
</ul>

<h2>Call For Papers</h2>
  <p class="mb-0"><b>Submission deadline:</b> October 26, 2018 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> November 12, 2018 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://cmt3.research.microsoft.com/SECML2018" target="_blank">https://cmt3.research.microsoft.com/SECML2018</a></p>
	
  <p>The workshop will include contributed papers. Based on the PC’s recommendation, each paper accepted to the workshop will be allocated either a contributed talk or poster presentation (UPDATE: spotlight presentations were removed from the schedule to make more time for poster sessions).</p>
  <p>There are two tracks for submissions:</p>
  <ul>
	  <li><b>Research Track:</b> Submissions to this track will introduce novel ideas or results. Submissions should follow the NeurIPS format and not exceed 4 pages (excluding references, appendices or large figures).</li>
	  <li><b>Encore Track:</b> Papers already accepted at other venues can be submitted to this track. There are no format constraints.</li>
  </ul>
  <p>We invite submissions on <b>any aspect of machine learning that relates to computer security (and vice versa)</b>. This includes, but is not limited to:</p>
  <ul>
	  <li>Training time attacks (e.g., data poisoning)</li>
	  <li>Test time attacks (e.g., adversarial examples, model stealing)</li>
	  <li>Cryptography for machine learning</li>
	  <li>Theoretical foundations of secure machine learning</li>
	  <li>Formal verification of machine learning systems</li>
	  <li>Identifying bugs in machine learning systems</li>
	  <li>Position papers raising new directions for secure machine learning</li>
  </ul>
  <p><b>We particularly welcome submissions that introduce novel datasets and/or organize competitions on novel datasets.</b>
  When relevant, submissions are encouraged to clearly state their <b>threat model</b>, release <b>open-source code</b> and take particular care in conducting <b>ethical research</b>. Reviewing will be performed in a <b>single-blind</b> fashion (reviewers will be anonymous but not authors). Reviewing criteria include (a) <b>relevance</b>, (b) <b>quality</b> of the methodology and experiments, (c) <b>novelty</b>.</p>
  <p>Note that submissions on privacy would be best submitted to the <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=10934" target="_blank">workshop dedicated to this topic</a>.</p>
  <p>This workshop will not have proceedings.</p>
  <p>Contact secml2018-org@googlegroups.com for any questions.</p>

</main>



</body></html>
